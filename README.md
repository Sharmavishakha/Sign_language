# ðŸ“Œ Project Description
This project aims to convert sign language gestures into text using a combination of MediaPipe for real-time hand landmark detection and a Random Forest classifier for gesture recognition.

MediaPipe extracts 21 hand landmarks, which are processed into feature vectors representing hand posture. These features are then passed into a trained Random Forest model to classify static sign language gestures such as alphabets or simple words. The corresponding text is displayed on-screen, enabling intuitive and accessible communication between sign language users and non-signers.

The system is lightweight, runs in real time via webcam input, and is designed with a focus on accuracy, accessibility, and low computational overhead.
<img width="340" alt="sign_language" src="https://github.com/user-attachments/assets/faa3fc53-e5e8-4859-af6d-49bf5cf3abe1" />
